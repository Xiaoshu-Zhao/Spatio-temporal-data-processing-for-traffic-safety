{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"sentiment-traffic-model-classifier.ipynb","provenance":[{"file_id":"1Ot1Y-c9xsJjCfTjdsvZQMsaUWcmXsAzb","timestamp":1625124346252}],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"d0f78385088743ff9629e4395216bbe2":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_d784591e349e4ec9ad9796c4f5026836","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_e4bce045e13a450e8b84d044b8f3321f","IPY_MODEL_5e037a80b33240abb6932abd8e39b797"]}},"d784591e349e4ec9ad9796c4f5026836":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e4bce045e13a450e8b84d044b8f3321f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_4b1ba48e0cc74e1e9835f7b5ee4df437","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":231508,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":231508,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_1394bed1d9b54562ba571a51b65b64b5"}},"5e037a80b33240abb6932abd8e39b797":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_90bfa162612c4304a506f30a7aafef6e","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 232k/232k [00:02&lt;00:00, 107kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_d34f0761975246d78d1d30ef0f1d479c"}},"4b1ba48e0cc74e1e9835f7b5ee4df437":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"1394bed1d9b54562ba571a51b65b64b5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"90bfa162612c4304a506f30a7aafef6e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"d34f0761975246d78d1d30ef0f1d479c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f0722c208e8b4633853afd0fd81b4ced":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_dcbe86b959564ea099f24ff11508d1aa","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_453fea463bce4d0d85f356def7eac98a","IPY_MODEL_ca20cffbcde84737add3021292fd5b01"]}},"dcbe86b959564ea099f24ff11508d1aa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"453fea463bce4d0d85f356def7eac98a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_56931fbe202b4136b1dbac75090db291","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":28,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":28,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_cf36b33edb234c7fa86dc38e301e2319"}},"ca20cffbcde84737add3021292fd5b01":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_7b3d8bbeb0cf4c34830a7090dc58d49a","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 28.0/28.0 [00:00&lt;00:00, 33.5B/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_0062eaeee9ae4ad6bb6d77e68ddd4256"}},"56931fbe202b4136b1dbac75090db291":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"cf36b33edb234c7fa86dc38e301e2319":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"7b3d8bbeb0cf4c34830a7090dc58d49a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"0062eaeee9ae4ad6bb6d77e68ddd4256":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f421ab0b15574579a60f394ca67c3635":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_a010d9d0090b49a9af71d07a7a3e8ae6","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_067b4aa169b64543bd45d893cf624b82","IPY_MODEL_29c6db61ad3c406fac1a315755463862"]}},"a010d9d0090b49a9af71d07a7a3e8ae6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"067b4aa169b64543bd45d893cf624b82":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_e105406bf1be4f43b4e294d135bda907","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":466062,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":466062,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_005750e742e24c5f98b9913e31ec0663"}},"29c6db61ad3c406fac1a315755463862":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_f402b014249e42a2ab79421ff3a8d3e9","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 466k/466k [00:00&lt;00:00, 2.23MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_0ccd7aadc02741628a7d79cf35e3f62f"}},"e105406bf1be4f43b4e294d135bda907":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"005750e742e24c5f98b9913e31ec0663":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f402b014249e42a2ab79421ff3a8d3e9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"0ccd7aadc02741628a7d79cf35e3f62f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"id":"jdSBX7XS94Cj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1625717157079,"user_tz":240,"elapsed":14676,"user":{"displayName":"Xiaoshu Zhao","photoUrl":"","userId":"04362737402921253975"}},"outputId":"88100960-205e-4538-9f1d-c5c522d67e76"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jKafizRU-a80","executionInfo":{"status":"ok","timestamp":1625717158848,"user_tz":240,"elapsed":125,"user":{"displayName":"Xiaoshu Zhao","photoUrl":"","userId":"04362737402921253975"}},"outputId":"1e9b8c5d-d59d-452b-8cb0-1474d5c71d5b"},"source":["cd /content/drive/MyDrive/Spatio-temporal data processing for traffic safety/nlp"],"execution_count":2,"outputs":[{"output_type":"stream","text":["/content/drive/MyDrive/Spatio-temporal data processing for traffic safety/nlp\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QH_DuXWR-dC5","executionInfo":{"status":"ok","timestamp":1625717160802,"user_tz":240,"elapsed":314,"user":{"displayName":"Xiaoshu Zhao","photoUrl":"","userId":"04362737402921253975"}},"outputId":"d561d9ca-9414-4998-c30e-458d9cf82be2"},"source":["!ls"],"execution_count":3,"outputs":[{"output_type":"stream","text":["sample_twitter_subway.csv\t\t  subway_binary_model.pt\n","sentiment-traffic-model-classifier.ipynb  traffic-model-classifier.ipynb\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"mvniYHTRhwpz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1625717182501,"user_tz":240,"elapsed":20650,"user":{"displayName":"Xiaoshu Zhao","photoUrl":"","userId":"04362737402921253975"}},"outputId":"722f8a92-4569-419f-9044-27adaeb58317"},"source":["!pip install unidecode\n","!pip install tweet-preprocessor\n","!pip install contractions\n","!pip install word2number\n","!pip install transformers"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Collecting unidecode\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/25/723487ca2a52ebcee88a34d7d1f5a4b80b793f179ee0f62d5371938dfa01/Unidecode-1.2.0-py2.py3-none-any.whl (241kB)\n","\r\u001b[K     |█▍                              | 10kB 19.6MB/s eta 0:00:01\r\u001b[K     |██▊                             | 20kB 19.9MB/s eta 0:00:01\r\u001b[K     |████                            | 30kB 16.1MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 40kB 14.2MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 51kB 8.6MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 61kB 8.2MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 71kB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 81kB 9.7MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 92kB 8.9MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 102kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 112kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 122kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 133kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 143kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 153kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 163kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 174kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 184kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 194kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 204kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 215kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 225kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 235kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 245kB 8.4MB/s \n","\u001b[?25hInstalling collected packages: unidecode\n","Successfully installed unidecode-1.2.0\n","Collecting tweet-preprocessor\n","  Downloading https://files.pythonhosted.org/packages/17/9d/71bd016a9edcef8860c607e531f30bd09b13103c7951ae73dd2bf174163c/tweet_preprocessor-0.6.0-py3-none-any.whl\n","Installing collected packages: tweet-preprocessor\n","Successfully installed tweet-preprocessor-0.6.0\n","Collecting contractions\n","  Downloading https://files.pythonhosted.org/packages/93/f4/0ec4a458e4368cc3be2c799411ecf0bc961930e566dadb9624563821b3a6/contractions-0.0.52-py2.py3-none-any.whl\n","Collecting textsearch>=0.0.21\n","  Downloading https://files.pythonhosted.org/packages/d3/fe/021d7d76961b5ceb9f8d022c4138461d83beff36c3938dc424586085e559/textsearch-0.0.21-py2.py3-none-any.whl\n","Collecting pyahocorasick\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7f/c2/eae730037ae1cbbfaa229d27030d1d5e34a1e41114b21447d1202ae9c220/pyahocorasick-1.4.2.tar.gz (321kB)\n","\u001b[K     |████████████████████████████████| 327kB 8.3MB/s \n","\u001b[?25hCollecting anyascii\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/14/666cd44bf53f36a961544af592cb5c5c800013f9c51a4745af8d7c17362a/anyascii-0.2.0-py3-none-any.whl (283kB)\n","\u001b[K     |████████████████████████████████| 286kB 46.8MB/s \n","\u001b[?25hBuilding wheels for collected packages: pyahocorasick\n","  Building wheel for pyahocorasick (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyahocorasick: filename=pyahocorasick-1.4.2-cp37-cp37m-linux_x86_64.whl size=85399 sha256=c1e21f56ac45ff13224b77ddda916e4eba8625136ca87e22d882148c4dfe265f\n","  Stored in directory: /root/.cache/pip/wheels/3a/03/34/77e3ece0bba8b86bfac88a79f923b36d805cad63caeba38842\n","Successfully built pyahocorasick\n","Installing collected packages: pyahocorasick, anyascii, textsearch, contractions\n","Successfully installed anyascii-0.2.0 contractions-0.0.52 pyahocorasick-1.4.2 textsearch-0.0.21\n","Collecting word2number\n","  Downloading https://files.pythonhosted.org/packages/4a/29/a31940c848521f0725f0df6b25dca8917f13a2025b0e8fcbe5d0457e45e6/word2number-1.1.zip\n","Building wheels for collected packages: word2number\n","  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for word2number: filename=word2number-1.1-cp37-none-any.whl size=5584 sha256=a7a1ab78540f40351686b1ee45e676bcabf3f3c25c69f9106661c63e45a90117\n","  Stored in directory: /root/.cache/pip/wheels/46/2f/53/5f5c1d275492f2fce1cdab9a9bb12d49286dead829a4078e0e\n","Successfully built word2number\n","Installing collected packages: word2number\n","Successfully installed word2number-1.1\n","Collecting transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fd/1a/41c644c963249fd7f3836d926afa1e3f1cc234a1c40d80c5f03ad8f6f1b2/transformers-4.8.2-py3-none-any.whl (2.5MB)\n","\u001b[K     |████████████████████████████████| 2.5MB 7.4MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.5.0)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from transformers) (3.13)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Collecting tokenizers<0.11,>=0.10.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/e2/df3543e8ffdab68f5acc73f613de9c2b155ac47f162e725dcac87c521c11/tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3MB)\n","\u001b[K     |████████████████████████████████| 3.3MB 47.8MB/s \n","\u001b[?25hCollecting huggingface-hub==0.0.12\n","  Downloading https://files.pythonhosted.org/packages/2f/ee/97e253668fda9b17e968b3f97b2f8e53aa0127e8807d24a547687423fe0b/huggingface_hub-0.0.12-py3-none-any.whl\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Collecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n","\u001b[K     |████████████████████████████████| 901kB 45.2MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n","Installing collected packages: tokenizers, huggingface-hub, sacremoses, transformers\n","Successfully installed huggingface-hub-0.0.12 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.8.2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"YFUrq49gilC7","executionInfo":{"status":"ok","timestamp":1625717190611,"user_tz":240,"elapsed":2381,"user":{"displayName":"Xiaoshu Zhao","photoUrl":"","userId":"04362737402921253975"}}},"source":["import spacy\n","import pandas as pd\n","import unidecode\n","from word2number import w2n\n","import contractions\n","import preprocessor as p\n","import numpy as np\n"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"H_XY898F64_8","executionInfo":{"status":"ok","timestamp":1625717193522,"user_tz":240,"elapsed":572,"user":{"displayName":"Xiaoshu Zhao","photoUrl":"","userId":"04362737402921253975"}}},"source":["sample_data_path = 'sample_twitter_subway.csv'\n","df_raw = pd.read_csv(sample_data_path)"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6V3UXTi68CQO"},"source":["# Preprocess\n","\n","## Text Cleaning\n","\n","Manual Elimination & Text Cleaning\n","\n","Remove Retweets 'RT @' from the dataset, to lowercase and Remove all 'Uber Eats' records\n"]},{"cell_type":"code","metadata":{"id":"Hb6Wf1qvRrPM","executionInfo":{"status":"ok","timestamp":1625717198618,"user_tz":240,"elapsed":130,"user":{"displayName":"Xiaoshu Zhao","photoUrl":"","userId":"04362737402921253975"}}},"source":["def remove_manually(df, word_list=['uber eats']):\n","    df = df[~df[\"txt\"].str.contains(\"RT @\")]\n","    df[\"txt\"] = df[\"txt\"].str.lower()\n","    for word in word_list:\n","        df = df[~df[\"txt\"].str.contains(word)]\n","    return df"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"Rf6WAVCaTjWe","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1625717767884,"user_tz":240,"elapsed":125,"user":{"displayName":"Xiaoshu Zhao","photoUrl":"","userId":"04362737402921253975"}},"outputId":"61d4c9e0-b981-46c8-f5eb-534dc090cf3d"},"source":["df = remove_manually(df_raw)\n","df = df.loc[df.sentiment.notnull(),:]\n","df.replace({'sentiment': {'h':2, 'm':1, 'l':0}}, inplace=True)"],"execution_count":33,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  This is separate from the ipykernel package so we can avoid doing imports until\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":419},"id":"xWGZjqXn65XM","executionInfo":{"status":"ok","timestamp":1625717770689,"user_tz":240,"elapsed":136,"user":{"displayName":"Xiaoshu Zhao","photoUrl":"","userId":"04362737402921253975"}},"outputId":"7549c014-3699-4f21-8337-6f3ac9e6a51d"},"source":["df"],"execution_count":34,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>txt</th>\n","      <th>label</th>\n","      <th>sentiment</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>9</th>\n","      <td>9</td>\n","      <td>@evanplewis after 14.5 months of silence, subw...</td>\n","      <td>y</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>17</td>\n","      <td>@ashindestad my mate used to say taking the tr...</td>\n","      <td>y</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>19</td>\n","      <td>@laurenjohnston after 14.5 months of silence, ...</td>\n","      <td>y</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>35</th>\n","      <td>35</td>\n","      <td>i was going to be mad about all the trains hom...</td>\n","      <td>y</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>59</th>\n","      <td>59</td>\n","      <td>@nyctsubway alas, now a person \"needing medica...</td>\n","      <td>y</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>308</th>\n","      <td>323</td>\n","      <td>@nyctsubway why are there no b or d trains goi...</td>\n","      <td>y</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>309</th>\n","      <td>324</td>\n","      <td>@nyctsubway what’s going on with the northboun...</td>\n","      <td>y</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>314</th>\n","      <td>331</td>\n","      <td>first time on the subway in 18 months and i fo...</td>\n","      <td>y</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>318</th>\n","      <td>335</td>\n","      <td>@idothethinking great thread!\\n\\non eliminatin...</td>\n","      <td>y</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>321</th>\n","      <td>338</td>\n","      <td>i literally boarded the same train i would hav...</td>\n","      <td>y</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>61 rows × 4 columns</p>\n","</div>"],"text/plain":["     Unnamed: 0  ... sentiment\n","9             9  ...         2\n","17           17  ...         2\n","19           19  ...         2\n","35           35  ...         2\n","59           59  ...         2\n","..          ...  ...       ...\n","308         323  ...         1\n","309         324  ...         1\n","314         331  ...         1\n","318         335  ...         0\n","321         338  ...         1\n","\n","[61 rows x 4 columns]"]},"metadata":{"tags":[]},"execution_count":34}]},{"cell_type":"markdown","metadata":{"id":"W5ZN0gL9RqpP"},"source":["Procedure\n","1.   Convert Accented Characters\n","2.   Expand Contractions\n","3.   Clean Text (using [tweet-preprocessor](https://github.com/s/preprocessor) package)\n","4.   Tokenize\n","\n"]},{"cell_type":"code","metadata":{"id":"nYEODQLv8Izp","executionInfo":{"status":"ok","timestamp":1625717208543,"user_tz":240,"elapsed":1061,"user":{"displayName":"Xiaoshu Zhao","photoUrl":"","userId":"04362737402921253975"}}},"source":["# https://towardsdatascience.com/nlp-text-preprocessing-a-practical-guide-and-template-d80874676e79\n","import en_core_web_sm\n","nlp = en_core_web_sm.load()\n","\n","# exclude words from spacy stopwords list\n","deselect_stop_words = ['no', 'not']\n","for w in deselect_stop_words:\n","    nlp.vocab[w].is_stop = False\n","\n","p.set_options(p.OPT.URL, p.OPT.MENTION, p.OPT.HASHTAG, p.OPT.EMOJI, p.OPT.SMILEY)\n","\n","\n","def remove_accented_chars(text):\n","    \"\"\"\n","    remove accented characters from text, e.g. café\n","    \"\"\"\n","    text = unidecode.unidecode(text)\n","    return text\n","\n","def expand_contractions(text):\n","    \"\"\"\n","    expand shortened words, e.g. don't to do not\n","    \"\"\"\n","    text = contractions.fix(text)\n","    return text\n","\n","def clean_txt(target_text):\n","\n","    target_text = remove_accented_chars(target_text)\n","    target_text = expand_contractions(target_text)\n","    target_text = p.clean(target_text)\n","\n","    doc = nlp(target_text)\n","    clean_text = []\n","    for token in doc:\n","        flag = True\n","        edit = token.text\n","        # remove stop words\n","        if token.is_stop and token.pos_ != 'NUM': \n","            flag = False\n","        # remove punctuations\n","        if  token.pos_ == 'PUNCT' and flag == True: \n","            flag = False\n","        # remove special characters\n","        if  token.pos_ == 'SYM' and flag == True: \n","            flag = False\n","        # remove numbers\n","        if (token.pos_ == 'NUM' or token.text.isnumeric()) and flag == True:\n","            flag = False\n","        # convert number words to numeric numbers\n","        if token.pos_ == 'NUM' and flag == True:\n","            edit = w2n.word_to_num(token.text)\n","        # convert tokens to base form\n","        elif token.lemma_ != \"-PRON-\" and flag == True:\n","            edit = token.lemma_\n","        # append tokens edited and not removed to list \n","        if edit != \"\" and flag == True:\n","            clean_text.append(edit)\n","            \n","    return \" \".join(clean_text)"],"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"k0SMIuaaqoPL"},"source":["## Word Embedding\n","\n"]},{"cell_type":"code","metadata":{"id":"0tXbw__xpWrt","colab":{"base_uri":"https://localhost:8080/","height":164,"referenced_widgets":["d0f78385088743ff9629e4395216bbe2","d784591e349e4ec9ad9796c4f5026836","e4bce045e13a450e8b84d044b8f3321f","5e037a80b33240abb6932abd8e39b797","4b1ba48e0cc74e1e9835f7b5ee4df437","1394bed1d9b54562ba571a51b65b64b5","90bfa162612c4304a506f30a7aafef6e","d34f0761975246d78d1d30ef0f1d479c","f0722c208e8b4633853afd0fd81b4ced","dcbe86b959564ea099f24ff11508d1aa","453fea463bce4d0d85f356def7eac98a","ca20cffbcde84737add3021292fd5b01","56931fbe202b4136b1dbac75090db291","cf36b33edb234c7fa86dc38e301e2319","7b3d8bbeb0cf4c34830a7090dc58d49a","0062eaeee9ae4ad6bb6d77e68ddd4256","f421ab0b15574579a60f394ca67c3635","a010d9d0090b49a9af71d07a7a3e8ae6","067b4aa169b64543bd45d893cf624b82","29c6db61ad3c406fac1a315755463862","e105406bf1be4f43b4e294d135bda907","005750e742e24c5f98b9913e31ec0663","f402b014249e42a2ab79421ff3a8d3e9","0ccd7aadc02741628a7d79cf35e3f62f"]},"executionInfo":{"status":"ok","timestamp":1625717218622,"user_tz":240,"elapsed":6955,"user":{"displayName":"Xiaoshu Zhao","photoUrl":"","userId":"04362737402921253975"}},"outputId":"fa748f99-d87d-4cef-de7c-ac0b81424c0d"},"source":["from transformers import BertTokenizer\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"],"execution_count":12,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d0f78385088743ff9629e4395216bbe2","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f0722c208e8b4633853afd0fd81b4ced","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=28.0, style=ProgressStyle(description_w…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f421ab0b15574579a60f394ca67c3635","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=466062.0, style=ProgressStyle(descripti…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ME_UuKV0uDFE","executionInfo":{"status":"ok","timestamp":1625717230736,"user_tz":240,"elapsed":119,"user":{"displayName":"Xiaoshu Zhao","photoUrl":"","userId":"04362737402921253975"}}},"source":["MAX_LEN = 110\n","def preprocessing_for_bert(data):\n","    \"\"\"Perform required preprocessing steps for pretrained BERT.\n","    @param    data (np.array): Array of texts to be processed.\n","    @return   input_ids (torch.Tensor): Tensor of token ids to be fed to a model.\n","    @return   attention_masks (torch.Tensor): Tensor of indices specifying which\n","                  tokens should be attended to by the model.\n","    \"\"\"\n","    # Create empty lists to store outputs\n","    input_ids = []\n","    attention_masks = []\n","\n","    # For every sentence...\n","    for sent in data:\n","        # `encode_plus` will:\n","        #    (1) Tokenize the sentence\n","        #    (2) Add the `[CLS]` and `[SEP]` token to the start and end\n","        #    (3) Truncate/Pad sentence to max length\n","        #    (4) Map tokens to their IDs\n","        #    (5) Create attention mask\n","        #    (6) Return a dictionary of outputs\n","        encoded_sent = tokenizer.encode_plus(\n","            text=clean_txt(sent),  # Preprocess sentence\n","            add_special_tokens = True,        # Add `[CLS]` and `[SEP]`\n","            max_length= MAX_LEN,                  # Max length to truncate/pad\n","            pad_to_max_length=True,         # Pad sentence to max length\n","            #return_tensors='pt',           # Return PyTorch tensor\n","            return_attention_mask=True      # Return attention mask\n","            )\n","        # Add the outputs to the lists\n","        input_ids.append(encoded_sent.get('input_ids'))\n","        attention_masks.append(encoded_sent.get('attention_mask'))\n","\n","    # Convert lists to tensors\n","    input_ids = torch.tensor(input_ids)\n","    attention_masks = torch.tensor(attention_masks)\n","\n","    return input_ids, attention_masks"],"execution_count":13,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xRyHd6JZhltN"},"source":["# Training"]},{"cell_type":"code","metadata":{"id":"GEAXarzXhXWY","executionInfo":{"status":"ok","timestamp":1625717780993,"user_tz":240,"elapsed":119,"user":{"displayName":"Xiaoshu Zhao","photoUrl":"","userId":"04362737402921253975"}}},"source":["y = df.sentiment\n","X = df.txt"],"execution_count":35,"outputs":[]},{"cell_type":"code","metadata":{"id":"aey_Xl12hlCc","executionInfo":{"status":"ok","timestamp":1625717782492,"user_tz":240,"elapsed":113,"user":{"displayName":"Xiaoshu Zhao","photoUrl":"","userId":"04362737402921253975"}}},"source":["from sklearn.model_selection import train_test_split\n","X_train, X_val, y_train, y_val =\\\n","    train_test_split(X, y, test_size = 0.3, random_state=2021)"],"execution_count":36,"outputs":[]},{"cell_type":"code","metadata":{"id":"q5xdddFQiwlm","executionInfo":{"status":"ok","timestamp":1625717783742,"user_tz":240,"elapsed":111,"user":{"displayName":"Xiaoshu Zhao","photoUrl":"","userId":"04362737402921253975"}}},"source":["import torch\n","cuda = torch.device('cuda')\n","device = torch.device(\"cuda\")"],"execution_count":37,"outputs":[]},{"cell_type":"code","metadata":{"id":"LSkG_fy-xqBu","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1625717786005,"user_tz":240,"elapsed":711,"user":{"displayName":"Xiaoshu Zhao","photoUrl":"","userId":"04362737402921253975"}},"outputId":"7e680f4c-8490-462a-ff46-dc4acddb24bc"},"source":["train_inputs, train_masks = preprocessing_for_bert(X_train)\n","val_inputs, val_masks = preprocessing_for_bert(X_val)\n","\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","\n","# Convert other data types to torch.Tensor\n","train_labels = torch.tensor(y_train.to_numpy())\n","val_labels = torch.tensor(y_val.to_numpy())\n","\n","# For fine-tuning BERT, the authors recommend a batch size of 16 or 32.\n","batch_size = 16\n","\n","# Create the DataLoader for our training set\n","train_data = TensorDataset(train_inputs, train_masks, train_labels)\n","train_sampler = RandomSampler(train_data)\n","train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n","\n","# Create the DataLoader for our validation set\n","val_data = TensorDataset(val_inputs, val_masks, val_labels)\n","val_sampler = SequentialSampler(val_data)\n","val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)"],"execution_count":38,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"2xqVRwOIjR87","executionInfo":{"status":"ok","timestamp":1625717715208,"user_tz":240,"elapsed":170,"user":{"displayName":"Xiaoshu Zhao","photoUrl":"","userId":"04362737402921253975"}}},"source":["import torch\n","import torch.nn as nn\n","from transformers import BertModel, AdamW, get_linear_schedule_with_warmup\n","\n","# Create the BertClassfier class\n","class BertClassifier(nn.Module):\n","\n","    def __init__(self, freeze_bert=False):\n","\n","        super(BertClassifier, self).__init__()\n","        # Specify hidden size of BERT, hidden size of our classifier, and number of labels\n","        D_in, H = 768, 50\n","\n","        # Instantiate BERT model\n","        self.bert = BertModel.from_pretrained('bert-base-uncased')\n","\n","        #Instantiate an one-layer feed-forward classifier\n","        self.classifier = nn.Sequential(\n","            nn.Linear(D_in, 60),\n","            nn.Dropout(0.2),\n","            nn.Tanh(),\n","            nn.Linear(60,1),\n","            nn.Sigmoid()\n","        )\n","        # self.classifier = nn.Sequential(\n","        #    nn.Linear(D_in, 1),\n","        #    nn.Sigmoid()\n","        # )\n","\n","        # Freeze the BERT model\n","        if freeze_bert:\n","            for param in self.bert.parameters():\n","                param.requires_grad = False\n","        \n","    def forward(self, input_ids, attention_mask):\n","        \"\"\"\n","\n","        \"\"\"\n","        # Feed input to BERT\n","        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n","        \n","        # Extract the last hidden state of the token `[CLS]` for classification task\n","        last_hidden_state_cls = outputs[0][:, 0, :]\n","\n","        # Feed input to classifier to compute logits\n","        logits = self.classifier(last_hidden_state_cls)\n","\n","        return logits\n","\n","def initialize_model(epochs=4):\n","    \"\"\"Initialize the Bert Classifier, the optimizer and the learning rate scheduler.\n","    \"\"\"\n","    # Instantiate Bert Classifier\n","    bert_classifier = BertClassifier(freeze_bert=False)\n","\n","    # Tell PyTorch to run the model on GPU\n","    bert_classifier.to(device)\n","\n","    # Create the optimizer\n","    optimizer = AdamW(bert_classifier.parameters(),\n","                      lr=1e-5,    # Default learning rate\n","                      eps=1e-5,    # Default epsilon value\n","                      weight_decay= 1)\n","\n","    # Total number of training steps\n","    total_steps = len(train_dataloader) * epochs\n","    print('total steps {}'.format(total_steps))\n","\n","    # Set up the learning rate scheduler\n","    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n","    return bert_classifier, optimizer, scheduler"],"execution_count":30,"outputs":[]},{"cell_type":"code","metadata":{"id":"e_Vt8EZMjsPk","executionInfo":{"status":"ok","timestamp":1625717717279,"user_tz":240,"elapsed":255,"user":{"displayName":"Xiaoshu Zhao","photoUrl":"","userId":"04362737402921253975"}}},"source":["import random\n","import time\n","\n","# Specify loss function\n","criterion = nn.BCELoss().to(device)\n","\n","def set_seed(seed_value=42):\n","    \"\"\"Set seed for reproducibility.\n","    \"\"\"\n","    random.seed(seed_value)\n","    np.random.seed(seed_value)\n","    torch.manual_seed(seed_value)\n","    torch.cuda.manual_seed_all(seed_value)\n","\n","def train(model, train_dataloader, val_dataloader=None, epochs=4, evaluation=False):\n","    \"\"\"Train the BertClassifier model.\n","    \"\"\"\n","    # Start training loop\n","    print(\"Start training...\\n\")\n","    for epoch_i in range(epochs):\n","        # =======================================\n","        #               Training\n","        # =======================================\n","        # Print the header of the result table\n","        print(f\"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Train Acc':^10} | {'Val Acc':^9} | {'Elapsed':^9}\")\n","        print(\"-\"*80)\n","\n","        # Measure the elapsed time of each epoch\n","        t0_epoch, t0_batch = time.time(), time.time()\n","\n","        # Reset tracking variables at the beginning of each epoch\n","        total_loss, batch_loss, batch_counts = 0, 0, 0\n","\n","        # Put the model into the training mode\n","        model.train()\n","\n","        # For each batch of training data...\n","        for step, batch in enumerate(train_dataloader):\n","            batch_counts +=1\n","            # Load batch to GPU\n","            b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n","\n","            # Zero out any previously calculated gradients\n","            model.zero_grad()\n","\n","            # Perform a forward pass. This will return logits.\n","            logits = model(b_input_ids, b_attn_mask)\n","\n","            # Compute loss and accumulate the loss values\n","            b_labels = b_labels.unsqueeze(1)\n","            # print(logits)\n","            # print(b_labels)\n","            loss = criterion(logits.double(), b_labels.double())\n","            # print(loss)\n","            batch_loss += loss.item()\n","            total_loss += loss.item()\n","\n","            # Perform a backward pass to calculate gradients\n","            loss.backward()\n","\n","            # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","            # Update parameters and the learning rate\n","            optimizer.step()\n","            scheduler.step()\n","\n","            # Print the loss values and time elapsed for every 20 batches\n","            if (step % 20 == 0 and step != 0) or (step == len(train_dataloader) - 1):\n","                # Calculate time elapsed for 20 batches\n","                time_elapsed = time.time() - t0_batch\n","\n","                # Print training results\n","                print(f\"{epoch_i + 1:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f} | {'-':^10} | {'-':^9} | {'-':^9} | {time_elapsed:^9.2f}\")\n","\n","                # Reset batch tracking variables\n","                batch_loss, batch_counts = 0, 0\n","                t0_batch = time.time()\n","\n","        # Calculate the average loss over the entire training data\n","        avg_train_loss = total_loss / len(train_dataloader)\n","\n","        print(\"-\"*80)\n","        # =======================================\n","        #               Evaluation\n","        # =======================================\n","        if evaluation == True:\n","            # After the completion of each training epoch, measure the model's performance\n","            # on our validation set.\n","            val_loss, val_accuracy = evaluate(model, val_dataloader)\n","            train_acc = evaluate(model, train_dataloader)[1]\n","            # Print performance over the entire training data\n","            time_elapsed = time.time() - t0_epoch\n","            \n","            print(f\"{epoch_i + 1:^7} | {'-':^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {train_acc:^9.2f} | {val_accuracy:^9.2f} | {time_elapsed:^9.2f}\")\n","            print(\"-\"*80)\n","        print(\"\\n\")\n","    \n","    torch.save(model.state_dict(), '/content/drive/MyDrive/Spatio-temporal data processing for traffic safety/nlp/subway_sentiment_model.pt')\n","    \n","\n","    print(\"Training complete!\")\n","\n","\n","def evaluate(model, val_dataloader):\n","    \"\"\"After the completion of each training epoch, measure the model's performance\n","    on our validation set.\n","    \"\"\"\n","    # Put the model into the evaluation mode. The dropout layers are disabled during\n","    # the test time.\n","    model.eval()\n","\n","    # Tracking variables\n","    val_accuracy = []\n","    val_loss = []\n","\n","    # For each batch in our validation set...\n","    for batch in val_dataloader:\n","        # Load batch to GPU\n","        b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n","\n","        # Compute logits\n","        with torch.no_grad():\n","            logits = model(b_input_ids, b_attn_mask)\n","\n","        # Compute loss\n","        b_labels = b_labels.unsqueeze(1)\n","        loss = criterion(logits.double(), b_labels.double())\n","        val_loss.append(loss.item())\n","\n","        # Get the predictions\n","\n","        preds = logits >= 0.5\n","\n","        # Calculate the accuracy rate\n","        accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n","        val_accuracy.append(accuracy)\n","\n","    # Compute the average accuracy and loss over the validation set.\n","    val_loss = np.mean(val_loss)\n","    val_accuracy = np.mean(val_accuracy)\n","\n","    return val_loss, val_accuracy"],"execution_count":31,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BrNWJWt-jt-z","executionInfo":{"status":"ok","timestamp":1625717810900,"user_tz":240,"elapsed":16616,"user":{"displayName":"Xiaoshu Zhao","photoUrl":"","userId":"04362737402921253975"}},"outputId":"3fe6e5f7-a4ff-4ccf-8b2b-7042e5c717df"},"source":["set_seed(12)    # Set seed for reproducibility\n","bert_classifier, optimizer, scheduler = initialize_model(epochs=20)\n","print('initialized')\n","train(bert_classifier, train_dataloader, val_dataloader, epochs=10, evaluation = True)"],"execution_count":39,"outputs":[{"output_type":"stream","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"],"name":"stderr"},{"output_type":"stream","text":["total steps 60\n","initialized\n","Start training...\n","\n"," Epoch  |  Batch  |  Train Loss  |  Val Loss  | Train Acc  |  Val Acc  |  Elapsed \n","--------------------------------------------------------------------------------\n","   1    |    2    |   0.695978   |     -      |     -     |     -     |   0.89   \n","--------------------------------------------------------------------------------\n","   1    |    -    |   0.695978   |  0.685375  |   52.50   |   52.08   |   1.31   \n","--------------------------------------------------------------------------------\n","\n","\n"," Epoch  |  Batch  |  Train Loss  |  Val Loss  | Train Acc  |  Val Acc  |  Elapsed \n","--------------------------------------------------------------------------------\n","   2    |    2    |   0.700227   |     -      |     -     |     -     |   0.83   \n","--------------------------------------------------------------------------------\n","   2    |    -    |   0.700227   |  0.640281  |   42.50   |   38.54   |   1.26   \n","--------------------------------------------------------------------------------\n","\n","\n"," Epoch  |  Batch  |  Train Loss  |  Val Loss  | Train Acc  |  Val Acc  |  Elapsed \n","--------------------------------------------------------------------------------\n","   3    |    2    |   0.649357   |     -      |     -     |     -     |   0.84   \n","--------------------------------------------------------------------------------\n","   3    |    -    |   0.649357   |  0.593831  |   25.00   |   29.17   |   1.27   \n","--------------------------------------------------------------------------------\n","\n","\n"," Epoch  |  Batch  |  Train Loss  |  Val Loss  | Train Acc  |  Val Acc  |  Elapsed \n","--------------------------------------------------------------------------------\n","   4    |    2    |   0.602209   |     -      |     -     |     -     |   0.84   \n","--------------------------------------------------------------------------------\n","   4    |    -    |   0.602209   |  0.539720  |   26.67   |   35.42   |   1.27   \n","--------------------------------------------------------------------------------\n","\n","\n"," Epoch  |  Batch  |  Train Loss  |  Val Loss  | Train Acc  |  Val Acc  |  Elapsed \n","--------------------------------------------------------------------------------\n","   5    |    2    |   0.539321   |     -      |     -     |     -     |   0.84   \n","--------------------------------------------------------------------------------\n","   5    |    -    |   0.539321   |  0.484451  |   25.42   |   32.29   |   1.27   \n","--------------------------------------------------------------------------------\n","\n","\n"," Epoch  |  Batch  |  Train Loss  |  Val Loss  | Train Acc  |  Val Acc  |  Elapsed \n","--------------------------------------------------------------------------------\n","   6    |    2    |   0.510015   |     -      |     -     |     -     |   0.84   \n","--------------------------------------------------------------------------------\n","   6    |    -    |   0.510015   |  0.435027  |   26.67   |   32.29   |   1.28   \n","--------------------------------------------------------------------------------\n","\n","\n"," Epoch  |  Batch  |  Train Loss  |  Val Loss  | Train Acc  |  Val Acc  |  Elapsed \n","--------------------------------------------------------------------------------\n","   7    |    2    |   0.520153   |     -      |     -     |     -     |   0.85   \n","--------------------------------------------------------------------------------\n","   7    |    -    |   0.520153   |  0.404851  |   25.83   |   32.29   |   1.28   \n","--------------------------------------------------------------------------------\n","\n","\n"," Epoch  |  Batch  |  Train Loss  |  Val Loss  | Train Acc  |  Val Acc  |  Elapsed \n","--------------------------------------------------------------------------------\n","   8    |    2    |   0.440296   |     -      |     -     |     -     |   0.86   \n","--------------------------------------------------------------------------------\n","   8    |    -    |   0.440296   |  0.385499  |   23.33   |   32.29   |   1.30   \n","--------------------------------------------------------------------------------\n","\n","\n"," Epoch  |  Batch  |  Train Loss  |  Val Loss  | Train Acc  |  Val Acc  |  Elapsed \n","--------------------------------------------------------------------------------\n","   9    |    2    |   0.478839   |     -      |     -     |     -     |   0.86   \n","--------------------------------------------------------------------------------\n","   9    |    -    |   0.478839   |  0.367640  |   20.83   |   32.29   |   1.29   \n","--------------------------------------------------------------------------------\n","\n","\n"," Epoch  |  Batch  |  Train Loss  |  Val Loss  | Train Acc  |  Val Acc  |  Elapsed \n","--------------------------------------------------------------------------------\n","  10    |    2    |   0.424838   |     -      |     -     |     -     |   0.86   \n","--------------------------------------------------------------------------------\n","  10    |    -    |   0.424838   |  0.352848  |   25.42   |   32.29   |   1.30   \n","--------------------------------------------------------------------------------\n","\n","\n","Training complete!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"vViHkMjpnloM","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1625158467985,"user_tz":240,"elapsed":131,"user":{"displayName":"Xiaoshu Zhao","photoUrl":"","userId":"04362737402921253975"}},"outputId":"fc7faa12-7210-46b6-baa0-2c74d2b737b5"},"source":["pwd"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'/content/drive/MyDrive/Spatio-temporal data processing for traffic safety/nlp'"]},"metadata":{"tags":[]},"execution_count":23}]},{"cell_type":"code","metadata":{"id":"fdPPDLglptJ-"},"source":["!export CUDA_LAUNCH_BLOCKING=1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Dtys9mMnptr-"},"source":["y.value_counts()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iVB7cZcAp0or"},"source":[""],"execution_count":null,"outputs":[]}]}